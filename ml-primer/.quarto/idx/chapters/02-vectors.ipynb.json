{"title":"VECTORS AND SIMILARITY","markdown":{"yaml":{"title":"VECTORS AND SIMILARITY"},"containsRefs":false,"markdown":"\n\n\n\n\n\n\nVectors are sets of numbers that represent something. \n\nYour location in the world is a good example of a vector. Each location in the world has two coordinates - longitude and latitude. These can be thought of as a 2-dimensional vector representation - the first dimension is longitude, the second is latitude.\n\nVectors have both direction and length, and using a simple mathematical operation called the $ dot product $ it's easy to determine how similar one vector is to another. Vectors that point in similar directions with similar lengths will have a high dot product, whereas those that point in opposite directions or have wildly different lenghts will have lower dot products.\n\nMachine Learning relies heavily on finding relatinoships between data and identifying similarities - for this reason the dot product is fundamental.\n\n**Dot Product Calculation**</p>\nIf I have two vectors $p$ and $q$, each with two features $(a1, b1)$ and $(a2, b2)$\n\ni.e. $\\mathbf{p} = \\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}$ and $\\mathbf{q} = \\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix}$\n\n**Dot product (geometric form)**</p>\n- $\\mathbf{p} \\cdot \\mathbf{q} = \\|\\mathbf{p}\\| \\|\\mathbf{q}\\| \\cos(\\theta)$\n\nwhere:</p>\n- $\\|\\mathbf{p}\\| = \\sqrt{a_1^2 + b_1^2}$\n- $\\|\\mathbf{q}\\| = \\sqrt{a_2^2 + b_2^2}$\n- $\\theta$ is the angle between the vectors.\n\n**Dot product (algebraic form):**</p>\n- $\\mathbf{p} \\cdot \\mathbf{q} = a_1 a_2 + b_1 b_2$\n\n\n\n\n**Conclusion**</p>\nVisually it's clear that vector v2 is most like vector v1 because it has a similar length and direction. v4, on the other hand, scores a negative dot product because it's pointing in a completely different direction.</p>\nThe key insight is that if we can represent real-world items as vectors, there's a simple mathematical formula to determine similarity. In fact, some early language models do exactly this: they represent individual words as vectors (one vector per word in the dictionary), they're then 'trained' on thousands of documents. During training, the vector representations of words are adjusted so that words with similar meanings end up with vectors whose dot product yields high values. For example, the word 'King' and 'Queen' would score high dotproducts whereas the word 'King' and 'Tree' wouldn't (unless many of the training documents somehow contained stories about Kings who spend a lot of time in forests!)</p>\nThis same principle — measuring similarity via dot products — underpins modern embedding techniques used in transformers and large language models today.\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\nVectors are sets of numbers that represent something. \n\nYour location in the world is a good example of a vector. Each location in the world has two coordinates - longitude and latitude. These can be thought of as a 2-dimensional vector representation - the first dimension is longitude, the second is latitude.\n\nVectors have both direction and length, and using a simple mathematical operation called the $ dot product $ it's easy to determine how similar one vector is to another. Vectors that point in similar directions with similar lengths will have a high dot product, whereas those that point in opposite directions or have wildly different lenghts will have lower dot products.\n\nMachine Learning relies heavily on finding relatinoships between data and identifying similarities - for this reason the dot product is fundamental.\n\n**Dot Product Calculation**</p>\nIf I have two vectors $p$ and $q$, each with two features $(a1, b1)$ and $(a2, b2)$\n\ni.e. $\\mathbf{p} = \\begin{pmatrix} a_1 \\\\ b_1 \\end{pmatrix}$ and $\\mathbf{q} = \\begin{pmatrix} a_2 \\\\ b_2 \\end{pmatrix}$\n\n**Dot product (geometric form)**</p>\n- $\\mathbf{p} \\cdot \\mathbf{q} = \\|\\mathbf{p}\\| \\|\\mathbf{q}\\| \\cos(\\theta)$\n\nwhere:</p>\n- $\\|\\mathbf{p}\\| = \\sqrt{a_1^2 + b_1^2}$\n- $\\|\\mathbf{q}\\| = \\sqrt{a_2^2 + b_2^2}$\n- $\\theta$ is the angle between the vectors.\n\n**Dot product (algebraic form):**</p>\n- $\\mathbf{p} \\cdot \\mathbf{q} = a_1 a_2 + b_1 b_2$\n\n\n\n\n**Conclusion**</p>\nVisually it's clear that vector v2 is most like vector v1 because it has a similar length and direction. v4, on the other hand, scores a negative dot product because it's pointing in a completely different direction.</p>\nThe key insight is that if we can represent real-world items as vectors, there's a simple mathematical formula to determine similarity. In fact, some early language models do exactly this: they represent individual words as vectors (one vector per word in the dictionary), they're then 'trained' on thousands of documents. During training, the vector representations of words are adjusted so that words with similar meanings end up with vectors whose dot product yields high values. For example, the word 'King' and 'Queen' would score high dotproducts whereas the word 'King' and 'Tree' wouldn't (unless many of the training documents somehow contained stories about Kings who spend a lot of time in forests!)</p>\nThis same principle — measuring similarity via dot products — underpins modern embedding techniques used in transformers and large language models today.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"02-vectors.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.27","theme":"darkly","title":"VECTORS AND SIMILARITY"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}