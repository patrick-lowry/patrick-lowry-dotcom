{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TODO</h3> \n",
    "Fill in any place that says <code># YOUR CODE HERE</code> (make sure to remove the line <code>raise NotImplementedError()</code>).\n",
    "\n",
    "<h3>Suggestions</h3>\n",
    "\n",
    "- To speed up your code, think about how certain operations can be done at the same time.\n",
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "- Double check your code does not have $\\infty$-loops, these will crash the autograder.\n",
    "\n",
    "<h3>Rules</h3>\n",
    "\n",
    "- Blank cells in the notebook are hidden tests. **Do not delete, copy, paste, or alter these cells as this will cause the tests to fail automatically**.\n",
    "- Do not create multiple python notebooks (.ipynb files).\n",
    "- Do not import any new python packages (this may cause hidden tests to fail).\n",
    "- Each cell must run for less than 5 minutes (there exists a solution with full marks).\n",
    "- **Do not plagiarise!** We take violations of this very seriously. In previous years we have identified instances of plagiarism and reported them to the Senior Teaching & Learning Administrator.\n",
    "- If you are happy with your current grade you do not need to resubmit, the most recent grade from the autograder will be your final grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrick Feb 2026:\n",
    "- I had to do pip3 install \"numpy<2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b50351bd7dbcf201e978920a53b2f029",
     "grade": false,
     "grade_id": "cell-4beed8a45acd7af1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "import l2distance\n",
    "from l2distance import l2distance\n",
    "import visclassifier\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "#new torch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#misc imports\n",
    "import random\n",
    "\n",
    "#%matplotlib widget\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e6df8d2920aaafcdae46b8af3ef674fb",
     "grade": false,
     "grade_id": "cell-5f452d4d6d57362d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "In this project, you will implement a linear support vector machine and one operating in kernel space. For this you will need to formulate the primal and dual optimization problems as quadratic programs. For this, we will be dipping into the shallow end with Course staffs' favorite ML framework: PyTorch!\n",
    "\n",
    "For full documentation and details, here is their site https://pytorch.org/. PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. Pytorch is very neat because, as you have seen your assignments, in order to do gradient descent we've had to calculate gradient manually. No more! Pytorch performs automatic differentation, as long as we use their functions in our code.\n",
    "\n",
    "Note: Because we are working with Pytorch functions and Modules, we will be using excusively Pytorch tensors instead of numpy arrays. This allows us to multiply/use torch parameter objects with our data directly. Pytorch tensors carry most of the same functionality as numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2a34c086ce4976a7da04e7a78719ef95",
     "grade": false,
     "grade_id": "cell-63c47f087db65e79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will start with a simple example of PyTorch, where we use gradient descent to find the parameters of a simple linear regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "140808b7b946f8b1e6423b7176a888bd",
     "grade": false,
     "grade_id": "cell-2b19f0dd3cb286da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gen_regression_data(num_samples = 10000, ndims=1):\n",
    "    # generate random x samples for training and test sets\n",
    "    xTr = torch.rand(num_samples, ndims)\n",
    "    xTe = torch.rand(int(num_samples * 0.1), ndims)\n",
    "    \n",
    "    # construct random w and b vectors\n",
    "    gt_w = torch.randn(ndims, 1)\n",
    "    gt_b = torch.randn(1)\n",
    "    \n",
    "    # gaussian noise for linear regression\n",
    "    # Replaced np.random.normal with torch.randn to avoid \"Numpy is not available\" error\n",
    "    # when mixing numpy arrays with torch tensors\n",
    "    # noise = np.random.normal(size=(num_samples, 1)) * 0.02\n",
    "    # test_noise = np.random.normal(size=(int(num_samples * 0.1), 1)) * 0.02\n",
    "    noise = torch.randn(num_samples, 1) * 0.02\n",
    "    test_noise = torch.randn(int(num_samples * 0.1), 1) * 0.02\n",
    "    \n",
    "    # add noise on the labels for the training set\n",
    "    yTr = xTr @ gt_w + gt_b + noise\n",
    "    yTe = xTe @ gt_w + gt_b + test_noise\n",
    "    \n",
    "    return xTr, xTe, yTr, yTe, gt_w, gt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "767d76436ce46c796db32b28b953a150",
     "grade": false,
     "grade_id": "cell-b7a42c529326718f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lr_xTr, lr_xTe, lr_yTr, lr_yTe, gt_w, gt_b = gen_regression_data(num_samples = 1000, ndims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26f38015a831aae01892c4b1750f0649",
     "grade": false,
     "grade_id": "cell-deee2359d73bbc08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we will create our PyTorch model. PyTorch models inherit the class torch.nn.module, and you need to implement the function forward which is equivalent to a forward pass. Usually, you feed in batch of x samples as input and you get batch of outputs, but you could pass other parameters as well if needed. Every torch module will implement two functions. __init__ its constructor, and __forward__ which defines what happens when you call the module.\n",
    "\n",
    "Note we define two fields of the <code>LinearRegressionModel</code>:\n",
    "* <code>self.w</code>: The weight vector of the linear regression model. This is updated automatically by Pytorch in our training loop because we define it as a nn.Parameter (note requires_grad=True). Additionally <code>torch.randn(ndims, 1)</code> gives us an initialization for this weight vector.\n",
    "* <code>self.b</code>: The bias of the linear regression model. This is also updated automatically by Pytorch in our training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d2590b6f31fbb59256891bba5f3307d3",
     "grade": false,
     "grade_id": "cell-815d937cc7591c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        \"\"\" pytorch optimizer checks for the properties of the model, and if\n",
    "            the torch.nn.Parameter requires gradient, then the model will update\n",
    "            the parameters automatically.\n",
    "        \"\"\"\n",
    "        self.w = nn.Parameter(torch.randn(dim, 1), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "419700018fdb6bddc7415cd48cd6ef87",
     "grade": false,
     "grade_id": "cell-a730c75f377a1928",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this example we use our familiar mean-squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5222540b9ec1c52279f3f399c680768d",
     "grade": false,
     "grade_id": "cell-b0306506b59f6a38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(y_pred, y_true):\n",
    "    square_diff = torch.square((y_pred-y_true))\n",
    "    mean_error = 0.5 * torch.mean(square_diff)\n",
    "    return mean_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b2f6ad69f265d37ca3370516393abccd",
     "grade": false,
     "grade_id": "cell-208d6a365e599862",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here is a generic training loop in Pytorch, this will be very useful for you in your assignment. We have supplied comments per line to help walk you through what each different part does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d48c4b95488f57513b47b6d5be7bc602",
     "grade": false,
     "grade_id": "cell-76795eebec829d46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_regression_model(xTr, yTr, num_epochs, reg_param, lr=1e-2, print_freq=100):\n",
    "    ndims = xTr.shape[1]\n",
    "    \n",
    "    model = LinearRegressionModel(ndims)  # initialize the model\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)  # create an SGD optimizer for the model parameters\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        optimizer.zero_grad()  \n",
    "        pred = model.forward(xTr)  # compute model predictions\n",
    "        loss = mse_loss(pred, yTr) + reg_param * torch.norm(model.w)\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        optimizer.step()  # performs a step of gradient descent\n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return model  # return trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.12165500223636627\n",
      "epoch 200 loss 0.010627716779708862\n",
      "epoch 300 loss 0.0017094126669690013\n",
      "epoch 400 loss 0.0009438351262360811\n",
      "epoch 500 loss 0.0008358262712135911\n",
      "epoch 600 loss 0.0007865844527259469\n",
      "epoch 700 loss 0.0007474968442693353\n",
      "epoch 800 loss 0.0007139502558857203\n",
      "epoch 900 loss 0.0006849425844848156\n",
      "epoch 1000 loss 0.0006598409381695092\n",
      "epoch 1100 loss 0.0006381183047778904\n",
      "epoch 1200 loss 0.0006193197914399207\n",
      "epoch 1300 loss 0.0006030512740835547\n",
      "epoch 1400 loss 0.0005889723543077707\n",
      "epoch 1500 loss 0.0005767882103100419\n",
      "epoch 1600 loss 0.0005662439507432282\n",
      "epoch 1700 loss 0.0005571192014031112\n",
      "epoch 1800 loss 0.0005492225755006075\n",
      "epoch 1900 loss 0.0005423888796940446\n",
      "epoch 2000 loss 0.0005364749813452363\n",
      "avg test error 0.00019607727881520987\n"
     ]
    }
   ],
   "source": [
    "model = train_regression_model(lr_xTr, lr_yTr, num_epochs=2000, reg_param=0.001, lr=1e-2)\n",
    "avg_test_error = mse_loss(model.forward(lr_xTe), lr_yTe)\n",
    "print('avg test error', avg_test_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6148658397e6f3eafb8b57e9e0e396c3",
     "grade": false,
     "grade_id": "cell-1224c53c0267d443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we have a trained model object that we can predict with by passing in input data via model.forward(x). Let's visualize how good of a fit our line is to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_xTr, model.forward(lr_xTr).detach(),linewidth=5.0, color=\"red\", label=\"Prediction Line\")\n",
    "plt.scatter(lr_xTr, lr_yTr, label=\"Train Points\")\n",
    "plt.scatter(lr_xTe, lr_yTe, label=\"Test Points\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7976a17ca6f98815e5fe983f12f422e1",
     "grade": false,
     "grade_id": "cell-c9f425965ba3cfab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the following assignment there are a bunch of PyTorch specfic functions that we believe will be very helpful for you. Those are:\n",
    "\n",
    "* <code>torch.clamp(input, min=None, max=None, *, out=None) </code>: Clamps all elements in input into the range [min, max]\n",
    "\n",
    "* <code>torch.sum(input, *, dtype=None) </code>: Returns the sum of all elements in the input tensor.\n",
    "\n",
    "* <code>torch.mean(input, *, dtype=None)</code>: Returns the mean value of all elements in the input tensor.\n",
    "\n",
    "* <code>torch.pow(input, exponent, *, out=None)</code>: Takes the power of each element in input with exponent and returns a tensor with the result.\n",
    "\n",
    "* <code>torch.exp(input, *, out=None)</code>: Returns a new tensor with the exponential of the elements of the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "83d725a07f043e45752f479126ef1889",
     "grade": false,
     "grade_id": "cell-cdefaa85e4682b67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3> Linear classification</h3>\n",
    "\n",
    "<p> The first part of the assignment is to implement a linear support vector machine. In order to do this, we are going to generate random data to classify:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "62333dc2402c1e11febda129998577e5",
     "grade": false,
     "grade_id": "cell-b99091c95c58a615",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def genrandomdata(n=100,b=0.):\n",
    "    # generate random data and linearly separable labels\n",
    "    xTr = np.random.randn(n, 2)\n",
    "    # defining random hyperplane\n",
    "    w0 = np.random.rand(2, 1)\n",
    "    # assigning labels +1, -1 labels depending on what side of the plane they lie on\n",
    "    yTr = np.sign(np.dot(xTr, w0)+b).flatten()\n",
    "    return torch.from_numpy(xTr).float(), torch.from_numpy(yTr).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2723cf8c87fcaa2f5afec7180393396c",
     "grade": false,
     "grade_id": "cell-f8347f448bc9596b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>Remember the SVM primal formulation\n",
    "$$\\begin{aligned}\n",
    "             &\\min_{\\mathbf{w},b,\\xi} \\|\\mathbf{w}\\|^2_2+C \\sum_{i=1}^n \\xi_i\\\\\n",
    "       & \\text{such that }  \\ \\forall i:\\\\\n",
    "             & y_i(\\mathbf{w}^\\top \\mathbf{x}_i+b)\\geq 1-\\xi_i\\\\\n",
    "             & \\xi_i\\geq 0.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "You will need to implement  the function <code>primalSVM</code>, which takes in training data <code>xTr</code> ($n\\times d$) and labels <code>yTr</code> ($n$) with <code>yTr[i]</code>$\\in \\{-1,1\\}$. Note that we aren't doing linear programming, this is gradient descent optimization so the constraints are something we do not worry about.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a50a8718afa1e61def9c95628d23b3f9",
     "grade": false,
     "grade_id": "cell-bdf6fc745851b343",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To warm up, implement <code>hinge_loss</code>, which calculates the loss described in $\\sum_{i=1}^n \\xi_i$. Working with torch tensors is a lot like working with numpy tensors, think about the best way to do tensor on tensor operations. <b>This method requires no loops</b>.\n",
    "\n",
    "Hint: <code>torch.clamp</code> might be useful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "00794893f936e464629dd72ab98a53a2",
     "grade": false,
     "grade_id": "cell-eb14e96379dad7ed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hinge_loss(y_pred, y_true):\n",
    "    # YOUR CODE HERE\n",
    "    # This is the hinge loss of predictions versus labels - the label is already provided\n",
    "    loss = torch.clamp( (1 - (y_pred * y_true)), 0)\n",
    "    return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ----------- MY TEST CELL ----------------\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msk\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01ml2distance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# my solution using numpy\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#\n",
    "# ----------- MY TEST CELL ----------------\n",
    "#\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as sk\n",
    "from l2distance import *\n",
    "\n",
    "#\n",
    "# my solution using numpy\n",
    "#\n",
    "def my_np_loss(y_pred, y_true):\n",
    "    total_loss = 0\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        res = 1 - (y_pred[i] * y_true[i]) \n",
    "        \n",
    "        if (res > 0):\n",
    "            loss = res\n",
    "        else:\n",
    "            loss = 0\n",
    "        \n",
    "        #print('label:', y_true[i], 'pred:', y_pred[i], 'res:', res, 'loss:', loss)\n",
    "        total_loss += loss\n",
    "    print('my_np total loss:', total_loss)\n",
    "\n",
    "\n",
    "#\n",
    "# solution I used at MITx\n",
    "#\n",
    "def pegasus_loss(y_pred, y_true):\n",
    "    n = y_true.shape[0]\n",
    "    #print('Pegasus with', n, 'predictions')\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i in range(n):\n",
    "        z = y_true[i] * y_pred[i]\n",
    "        if z>= 1:\n",
    "            hinge_loss = 0\n",
    "        else:\n",
    "            hinge_loss = 1 - z\n",
    "        total_loss += hinge_loss\n",
    "    print('pegasus total loss:', total_loss)\n",
    "\n",
    "#\n",
    "# using pytorch\n",
    "# \n",
    "def torch_hl(y_pred, y_true):\n",
    "    torch_loss = nn.HingeEmbeddingLoss()\n",
    "    py_loss = torch_loss(y_pred, y_true)    \n",
    "    print('torch loss is:', py_loss, '\\n')\n",
    "    \n",
    "torch_pred = torch.Tensor([0.97, 1.2, 0, -0.25, -0.88, -1.01, -0, 0.4])\n",
    "torch_true = torch.Tensor([1,  1, 1, 1, -1, -1, -1, -1])\n",
    "torch_loss = torch_hl(torch_pred, torch_true)    \n",
    "\n",
    "#\n",
    "# try the solution I wrote\n",
    "#\n",
    "cw2_loss = hinge_loss(torch_pred, torch_true)\n",
    "print('cw2 loss:', cw2_loss)\n",
    "\n",
    "# use stadard numpy\n",
    "y_pred = np.array([0.97, 1.2, 0, -0.25, -0.88, -1.01, -0, 0.4])\n",
    "y_true = np.array([1,  1, 1, 1, -1, -1, -1, -1])\n",
    "my_loss = my_np_loss(y_pred, y_true)\n",
    "\n",
    "# try MITx Pegasus\n",
    "pegasus_loss(y_pred, y_true)\n",
    "\n",
    "# check\n",
    "#answer = .03 + 0 +1 +1.25 +0.12 + 0+ 1 +1.4\n",
    "#print('\\ncorrect:', answer)\n",
    "\n",
    "\n",
    "# check L2 Sq Norm\n",
    "print('\\n L2 Sq Norm -----')\n",
    "weights = torch.Tensor([[1],[-2], [3]])\n",
    "weights_sq = torch.pow(weights, 2)\n",
    "w_sq_l2norm = torch.sum(weights_sq)\n",
    "print('w:\\n', weights)\n",
    "print('\\nl2sqnorm:\\n', w_sq_l2norm)\n",
    "print('\\nl2dist w.w:\\n',l2distance(weights,weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ef84adb5897d8abb4544789834245d13",
     "grade": false,
     "grade_id": "cell-d53bd9cf6a97a750",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, implement <code>LinearSVM</code>. This is a module (similar to the one in the example above) which initializes a linear classifer in dimension <code>dim</code>. In this module, you will need to initialize the necessary parameters for a linear model and define the forward pass for an input x. Hint: It <b>should</b> look very similar to what you have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b7ccf753a1f1ce2b3dfe23d90c9e1fef",
     "grade": false,
     "grade_id": "cell-767cf779d2d4b68e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"Support Vector Machine\"\"\"\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        # YOUR CODE HERE\n",
    "        super(LinearClassifier, self).__init__()\n",
    "        \n",
    "        # Need a vector for dim number of weights (w) and an intercept (b)\n",
    "        self.w = nn.Parameter(torch.randn(dim, 1), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n",
    "\n",
    "    # forward pass - creates the prediction\n",
    "    def forward(self, x):\n",
    "        return (x @ self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e004ffa096f6fcbe242e8ab305cd9ebf",
     "grade": false,
     "grade_id": "cell-bd3f1e535d4f9594",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, implement <code>primalSVM</code>. This is a method which takes in a set of training data <code>xTr</code> and labels <code>yTr</code>, a number of epochs <code>num_epochs</code> to train for, and our SVM <code>C</code> hyper-parameter. You should return a lambda function (https://www.w3schools.com/python/python_lambda.asp) <code>svmclassify</code> that produces a forward pass of your trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7be318e02f13aab8e4c64a234ecebc59",
     "grade": false,
     "grade_id": "cell-a1cbe4d52e9d7c21",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def primalSVM(xTr, yTr, num_epochs=1000, C=1):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    lr = 1e-3\n",
    "    print_freq=100\n",
    "    dim = xTr.shape[1] # get the number of dimensions\n",
    "    \n",
    "    # create my model and create SGD Optimizer\n",
    "    svmclassify = LinearClassifier(dim)\n",
    "    optimizer   = optim.SGD(svmclassify.parameters(), lr=lr)\n",
    "    \n",
    "    N = yTr.shape[0] # num samples\n",
    "    total_loss = 0\n",
    "    batch_size = 15\n",
    "    \n",
    "    svmclassify.train() # not sure if this makes any difference\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #random list of integers up to N - this will be regenerated each epoch\n",
    "        batch_indices = torch.randperm(N) \n",
    "        #for i in range(0, N, batch_size):\n",
    "        for i in range(N):\n",
    "            \n",
    "            idx = batch_indices[i]\n",
    "            curr_x = xTr[idx]\n",
    "            curr_y = yTr[idx]\n",
    "            \n",
    "            #print('Epoch', epoch, 'N', N, 'batch', i, 'to', i-1 + batch_size)\n",
    "            #if i == 0:\n",
    "            #    end = 4\n",
    "            #else:\n",
    "            #    end = i-1 + batch_size\n",
    "            \n",
    "            #curr_x = xTr[batch_indices[i : end]]\n",
    "            #curr_y = yTr[batch_indices[i : i-1 + end]]\n",
    "            \n",
    "            #curr_x = xTr[idx]\n",
    "            #curr_y = yTr[idx]\n",
    "\n",
    "            # L2Norm Squared\n",
    "            weights = svmclassify.w\n",
    "            \n",
    "            # get the predictions for x\n",
    "            pred = svmclassify.forward(curr_x)\n",
    "            h_loss = hinge_loss(pred, curr_y)\n",
    "            \n",
    "            l2_regulariser = weights.T @ weights\n",
    "            #weights_sq = torch.pow(weights, 2)\n",
    "            #sum_weights_sq = torch.sum(weights_sq)\n",
    "            \n",
    "            # update loss according to provided instructions\n",
    "            loss = (C * h_loss) + l2_regulariser\n",
    "            \n",
    "            loss.backward()  # compute the gradient wrt loss\n",
    "            optimizer.step()  # performs a step of gradient descent\n",
    "            optimizer.zero_grad()\n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    return svmclassify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# MY ALTERNATIVE\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "def hinge_loss_2(y_pred, y_true):\n",
    "    # YOUR CODE HERE\n",
    "    # This is the hinge loss of predictions versus labels - the label is already provided\n",
    "    \n",
    "    # change for 21 Feb - no max, just set minimum to zero\n",
    "    y_pred_new = torch.squeeze(y_pred)\n",
    "    z    = 1 - (y_pred_new * y_true)\n",
    "    losses  = torch.clamp(z, 0)\n",
    "    \n",
    "    return torch.sum(losses)\n",
    "\n",
    "\n",
    "def primalSVM_2(xTr, yTr, num_epochs=1000, C=1):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    lr = 1e-2\n",
    "    reg_param = 0.001\n",
    "    print_freq=100\n",
    "    ndims = xTr.shape[1] # get the number of dimensions\n",
    "    \n",
    "    # create my model and create SGD Optimizer\n",
    "    svmclassify = LinearClassifier(ndims)\n",
    "    optimizer   = optim.SGD(svmclassify.parameters(), lr=lr)\n",
    "    \n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        \n",
    "        # get the predictions for x\n",
    "        pred = svmclassify.forward(xTr)\n",
    "        hinge_loss = hinge_loss_2(pred, yTr)\n",
    "        \n",
    "        #weights = svmclassify.w.squeeze()\n",
    "        weights = svmclassify.w\n",
    "        \n",
    "        # calculate squared L2 Norm\n",
    "        l2_distance = l2distance(weights, weights)\n",
    "        l2_distance_sq = torch.pow(l2_distance, 2)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = (C * hinge_loss) + torch.sum(l2_distance_sq) #similar to above\n",
    "\n",
    "        loss.backward()  # compute the gradient wrt loss\n",
    "        optimizer.step()  # performs a step of gradient descent\n",
    "        \n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return svmclassify  # return trained model\n",
    "\n",
    "\n",
    "\n",
    "def primalSVM_3(xTr, yTr, num_epochs=1000, C=1):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    lr = 1e-2\n",
    "    reg_param = 0.001\n",
    "    print_freq=100\n",
    "    ndims = xTr.shape[1] # get the number of dimensions\n",
    "    \n",
    "    # create my model and create SGD Optimizer\n",
    "    svmclassify = LinearClassifier(ndims)\n",
    "    optimizer   = optim.SGD(svmclassify.parameters(), lr=lr)\n",
    "    \n",
    "    N = yTr.shape[0] # numnber of items\n",
    "    total_loss = 0\n",
    "    batch_size = 20\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # need to zero the gradients in the optimizer so we don't\n",
    "        # use the gradients from previous iterations\n",
    "        \n",
    "        perm = torch.randperm(N)\n",
    "    \n",
    "        for i in range(0, N, batch_size):\n",
    "            x_batch = xTr[perm[i : i + batch_size]]\n",
    "            y_batch = yTr[perm[i : i + batch_size]]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            pred = svmclassify.forward(x_batch)\n",
    "            hinge_loss = hinge_loss_2(pred, y_batch)\n",
    "            \n",
    "            #weights = svmclassify.w.squeeze()\n",
    "            weights = svmclassify.w\n",
    "        \n",
    "            # calculate squared L2 Norm\n",
    "            l2_distance = l2distance(weights, weights)\n",
    "            l2_distance_sq = torch.pow(l2_distance, 2)\n",
    "        \n",
    "            # calculate the loss\n",
    "            loss = (C * hinge_loss) + l2_distance_sq #similar to above\n",
    "\n",
    "            loss.backward()  # compute the gradient wrt loss\n",
    "            optimizer.step()  # performs a step of gradient descent\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            \n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "    \n",
    "    return svmclassify  # return trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f6a08a5830307ee949585f33435287e",
     "grade": false,
     "grade_id": "cell-54f91155ad52bf49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can test your SVM primal solver with the following randomly generated data set. We label it in a way that it is guaranteed to be linearly separable. If your code works correctly the hyper-plane should separate all the $x$'s into the red half and all the $o$'s into the blue half. With sufficiently large values of $C$ (e.g. $C>10$) you should obtain $0\\%$ training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "942f15027724330b08b344e668853a21",
     "grade": false,
     "grade_id": "cell-5178de8375ab9209",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr = genrandomdata()\n",
    "fun = primalSVM(xTr,yTr,1000, C=100)\n",
    "visclassifier.visclassifier(fun,xTr,yTr)\n",
    "#err=torch.mean((torch.sign(fun(xTr))!=yTr).float())\n",
    "err=torch.mean((torch.sign(fun(xTr).flatten())!=yTr).float())\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "11abf96dc4f7de3ebf35cbb6a4c3098f",
     "grade": true,
     "grade_id": "cell-87d9677909636fa7",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 1: testCase_Primal\n",
    "# ------------------------------\n",
    "# Given a fixed training set, this tests if the signs of predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdfc1e20da5fe9690bb2f080f49fe20b",
     "grade": true,
     "grade_id": "cell-64ae8dc140286cbb",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 2: testCase_Primal\n",
    "# ------------------------------\n",
    "# Given a fixed training set, this tests if points farther from the decision boundary have larger predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f85ff9f9468f6188a1868a9ed424c9f1",
     "grade": false,
     "grade_id": "cell-fc6595df8ebc2f14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Spiral data set</h3>\n",
    "\n",
    "<p>The linear classifier works great in simple linear cases. But what if the data is more complicated? We provide you with a \"spiral\" data set. You can load it and visualize it with the following two code snippets:\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa6c57ab812d72e68303666a968b86f9",
     "grade": false,
     "grade_id": "cell-b707721c5c88ff46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def spiraldata(N=300):\n",
    "    r = np.linspace(1,2*np.pi,N)\n",
    "    xTr1 = np.array([np.sin(2.*r)*r, np.cos(2*r)*r]).T\n",
    "    xTr2 = np.array([np.sin(2.*r+np.pi)*r, np.cos(2*r+np.pi)*r]).T\n",
    "    xTr = np.concatenate([xTr1, xTr2], axis=0)\n",
    "    yTr = np.concatenate([np.ones(N), -1 * np.ones(N)])\n",
    "    xTr = xTr + np.random.randn(xTr.shape[0], xTr.shape[1])*0.2\n",
    "    \n",
    "    xTe = xTr[::2,:]\n",
    "    yTe = yTr[::2]\n",
    "    xTr = xTr[1::2,:]\n",
    "    yTr = yTr[1::2]\n",
    "    \n",
    "    xTr = torch.tensor(xTr).float()\n",
    "    yTr = torch.tensor(yTr).float()\n",
    "    xTe = torch.tensor(xTe).float()\n",
    "    yTe = torch.tensor(yTe).float()\n",
    "    \n",
    "    vals, indices = torch.max(xTr, dim=0, keepdim=True)\n",
    "    xTr /= (vals * 2.0)\n",
    "    vals, indices = torch.max(xTe, dim=0, keepdim=True)\n",
    "    xTe /= (vals * 2.0)\n",
    "    \n",
    "    return xTr,yTr,xTe,yTe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "09178ec0a696bdf2f2b3266681ea839e",
     "grade": false,
     "grade_id": "cell-09fd0d13d248f578",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "plt.figure()\n",
    "plt.scatter(xTr[yTr == 1, 0], xTr[yTr == 1, 1], c='b')\n",
    "plt.scatter(xTr[yTr != 1, 0], xTr[yTr != 1, 1], c='r')\n",
    "plt.legend([\"+1\",\"-1\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79d94908d0146d80d4bc0275819987ef",
     "grade": false,
     "grade_id": "cell-0868475fb5c266a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>If you apply your previously functioning linear classifier on this data set you will see that you get terrible results. Your training error will increase drastically. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5994ef86bb9ea4809d2e782c2634fc83",
     "grade": false,
     "grade_id": "cell-7d1ebe38c2b8d1c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fun=primalSVM(xTr,yTr,C=10)\n",
    "visclassifier.visclassifier(fun,xTr,yTr)\n",
    "err=torch.mean(((torch.sign(fun(xTr)))!=yTr).float())\n",
    "print(\"Training error: %2.1f%%\" % (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4b6a7a90983f38ec958400059de75794",
     "grade": false,
     "grade_id": "cell-c7f79f4f5d5e55fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Implementing a kernelized SVM</h3>\n",
    "\n",
    "<p> For a data set as complex as the spiral data set, you will need a more complex classifier. \n",
    "First implement the kernel function\n",
    "<pre>\tcomputeK(kernel_type,X,Z,kpar)</pre>\n",
    "It takes as input a kernel type <code>kernel_type</code> and two data sets $\\mathbf{X} \\in \\mathcal{R}^{n\\times d}$ and $\\mathbf{Z} \\in \\mathcal{R}^{m\\times d}$ and outputs a kernel matrix $\\mathbf{K}\\in{\\mathcal{R}^{n\\times m}}$. The last input, <code>kpar</code> specifies the kernel parameter (e.g. the inverse kernel width $\\gamma$ in the RBF case or the degree $p$ in the polynomial case.)\n",
    "\t<ol>\n",
    "\t<li>For the linear kernel (<code>kernel_type='linear'</code>) svm, use $k(\\mathbf{x},\\mathbf{z})=x^Tz$ </li> \n",
    "\t<li>For the radial basis function kernel (<code>kernel_type='rbf'</code>) svm use $k(\\mathbf{x},\\mathbf{z})=\\exp(-\\gamma ||x-z||^2)$ (gamma is a hyperparameter, passed as the value of kpar)</li>\n",
    "\t<li>For the polynomial kernel (<code>kernel_type='poly'</code>) use  $k(\\mathbf{x},\\mathbf{z})=(x^Tz + 1)^d$ (d is the degree of the polymial, passed as the value of kpar)</li>\n",
    "</ol>\n",
    "\n",
    "<p>You can use the function <b><code>l2distance</code></b> as a helperfunction, which is located in defined in one of your starter files l2distance.py.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ffaffe8f8033a6bd84dee60ea7bb1838",
     "grade": false,
     "grade_id": "cell-948d761b9f7671fe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def computeK(kernel_type, X, Z, kpar=0):\n",
    "    \"\"\"\n",
    "    function K = computeK(kernel_type, X, Z)\n",
    "    computes a matrix K such that Kij=k(x,z);\n",
    "    for three different function linear, rbf or polynomial.\n",
    "    \n",
    "    Input:\n",
    "    kernel_type: either 'linear','polynomial','rbf'\n",
    "    X: n input vectors of dimension d (nxd);\n",
    "    Z: m input vectors of dimension d (mxd);\n",
    "    kpar: kernel parameter (inverse kernel width gamma in case of RBF, degree in case of polynomial)\n",
    "    \n",
    "    OUTPUT:\n",
    "    K : nxm kernel Torch float tensor\n",
    "    \"\"\"\n",
    "    assert kernel_type in [\"linear\",\"polynomial\",\"poly\",\"rbf\"], \"Kernel type %s not known.\" % kernel_type\n",
    "    assert X.shape[1] == Z.shape[1], \"Input dimensions do not match\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #print('computeK:', kernel_type, 'X:', X.shape, 'Z:', Z.shape, 'kpar:', kpar )\n",
    "\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    m = Z.shape[0]\n",
    "    \n",
    "    K = torch.empty((n,m))\n",
    "    \n",
    "    # linear: k(x,z) = X.T@Z - but Z may have more/less rows than X\n",
    "    if (kernel_type == 'linear'):\n",
    "        #K = X @ Z\n",
    "        K = X @ Z.T\n",
    "        return K\n",
    "    \n",
    "    # poly: Z may have less rows - thus do X@Z.T not X.T@Z\n",
    "    elif (kernel_type == 'poly' or kernel_type == 'polynomial'):        \n",
    "        K = torch.pow(1 + X @ Z.T, kpar)\n",
    "        return K\n",
    "    # rbf\n",
    "    elif (kernel_type == 'rbf'):        \n",
    "        dist_x_z = l2distance(X, Z)\n",
    "        exp_param = -kpar * (torch.pow(dist_x_z, 2))\n",
    "        K = torch.exp(exp_param)\n",
    "        return K\n",
    "    else:\n",
    "        return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ec6cb1bc06d8c89472292cb2688d170",
     "grade": true,
     "grade_id": "cell-caecc234e3612655",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 3: testCase_computeK_linear\n",
    "# ---------------------------------------\n",
    "# This tests whether the linear kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4208a4200946887dd4f4b7a9f1d6b362",
     "grade": true,
     "grade_id": "cell-15181fab6eaee5a1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 4: testCase_computeK_polynomial\n",
    "# -------------------------------------------\n",
    "# This tests whether the polynomial kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3f2d3f1abdf7cd2d818877935037f009",
     "grade": true,
     "grade_id": "cell-8cd4b6888cf1b058",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 5: testCase_computeK_rbf\n",
    "# ------------------------------------\n",
    "# This tests whether the rbf kernel is computed properly on an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>My Tests on computeK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_poly_K(X, Y, c, p):\n",
    "    \"\"\"\n",
    "        Compute the polynomial kernel between two matrices X and Y::\n",
    "            K(x, y) = (<x, y> + c)^p\n",
    "        for each pair of rows x in X and y in Y.\n",
    "\n",
    "        Args:\n",
    "            X - (n, d) NumPy array (n datapoints each with d features)\n",
    "            Y - (m, d) NumPy array (m datapoints each with d features)\n",
    "            c - an coefficient to trade off high-order and low-order terms (scalar)\n",
    "            p - the degree of the polynomial kernel\n",
    "\n",
    "        Returns:\n",
    "            kernel_matrix - (n, m) Numpy array containing the kernel matrix\n",
    "    \"\"\"\n",
    "    print('\\nmy_poly_k')\n",
    "    K = X @ Y.T\n",
    "    K += c\n",
    "    K **= p\n",
    "    return K\n",
    "\n",
    "\n",
    "X = torch.Tensor([[1,1], [2,2], [3, 3]])\n",
    "Z = torch.Tensor([[1,1], [2,2], [3,3], [4,4]])\n",
    "\n",
    "\n",
    "print('----\\nX:\\n', X, '\\nZ:\\n', Z,'\\n----')\n",
    "\n",
    "K = computeK(\"linear\", X, Z, 100)\n",
    "print('linear:\\n',K)\n",
    "\n",
    "K = computeK(\"poly\", X, Z, 2)\n",
    "print('poly:\\n', K)\n",
    "\n",
    "K = my_poly_K(X,Z,1,2)\n",
    "print(K)\n",
    "\n",
    "K = computeK(\"rbf\", X, Z, 1)\n",
    "print('rbf:\\n', K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ccf7ce40c01576ddcbbb800c93ff777",
     "grade": false,
     "grade_id": "cell-c1ebb4aee30b7b58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Additional Testing</h3>\n",
    "<p>The following code snippet plots an image of the kernel matrix for the data points in the spiral set. Use it to test your <b><code>computeK</code></b> function:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "K=computeK(\"rbf\",xTr,xTr,kpar=0.05)\n",
    "# plot an image of the kernel matrix\n",
    "plt.figure()\n",
    "plt.pcolormesh(K, cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1c7d6eb015dbf9b004eb3b34daf92141",
     "grade": false,
     "grade_id": "cell-6b72343891248ae1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Remember that the SVM optimization has the following dual formulation: (1)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "             &\\min_{\\alpha_1,\\cdots,\\alpha_n}\\frac{1}{2} \\sum_{i,j}\\alpha_i \\alpha_j y_i y_j \\mathbf{K}_{ij} - \\sum_{i=1}^{n}\\alpha_i  \\\\\n",
    "       \\text{s.t.}  &\\quad 0 \\leq \\alpha_i \\leq C\\\\\n",
    "             &\\quad \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "This is equivalent to solving for the SVM primal (2)\n",
    "$$ L(\\mathbf{w},b) = C\\sum_{i=1}^n \\max(1-y_i(\\mathbf{w}^\\top\\phi(\\mathbf{x}_i)+b),0) + ||w||_2^2$$\n",
    "where $\\mathbf{w}=\\sum_{i=1}^n y_i \\alpha_i \\phi(\\mathbf{x}_i)$ and $\\mathbf{K}_{ij}=k(\\mathbf{x}_i,\\mathbf{x}_j)=\\phi(\\mathbf{x}_i)^\\top\\phi(\\mathbf{x}_j)$, for some mapping $\\phi(\\cdot)$. However, after a change of variable, with $\\beta_i = \\alpha_iy_i$ and $\\beta \\in R^n$, (2) can be rewritten as follows (see https://arxiv.org/pdf/1404.1066.pdf for details):\n",
    "$$ min_{\\beta, b} \\frac{1}{2}\\beta^\\top K\\beta + \\frac{C}{2}\\sum_{i=1}^n {[\\max(1-y_i(\\beta^\\top k_i+b),0)]}^2$$\n",
    "where $k_i$ is the kernel matrix row corresponding to the ith training example. Notice that there are two relaxations: 1. the $\\beta_i$ are unconstrained, in contrast to $\\alpha_i$ in (1), which must satisfy $0 \\leq \\alpha_i \\leq C$; and 2. the squared hinge loss is used in place of the more common absolute hinge loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b88370e20b4250f895bd19b2be93614f",
     "grade": false,
     "grade_id": "cell-478923c9beb252f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the module \n",
    "    <pre>\n",
    "    KernelizedSVM(dim, kernel_type, kpar=0)\n",
    "    </pre>\n",
    "    This is a kernelized version of the SVM as defined above, which must maintain some kind of internal parameters for beta and b (hint: think what <code>dim</code> should be as a function of our training data) should be used for. Further, you are given <code>kernel_type</code> and <code>kpar</code>, which you should use in the creation of kernels by means of the method you wrote above <code>computeK</code>. For the forward pass of the kernelized SVM, recall that it is defined as $h(x) = w^\\top \\phi(x) + b$, where $w = \\sum_{i=1}^n \\beta_i\\phi(x_i)$. The output of your forward pass should be the classification itself of input data x.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "134b4b10c2005676c4399c0717a8a636",
     "grade": false,
     "grade_id": "cell-a6fae38b01273eed",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KernelizedSVM(nn.Module):\n",
    "    def __init__(self, dim, kernel_type, kpar=0):\n",
    "        # YOUR CODE HERE\n",
    "        super(KernelizedSVM, self).__init__()\n",
    "        \n",
    "        # variables that are update by the model\n",
    "        #self.beta = nn.Parameter(torch.zeros(dim), requires_grad=True) # Beta is basically one alpha per data point      \n",
    "        #self.b = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "        self.beta = nn.Parameter(torch.zeros(dim), requires_grad=True) # Beta is basically one alpha per data point      \n",
    "        self.b = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        \n",
    "        self.kernel_type = kernel_type\n",
    "        self.kpar = kpar\n",
    "        \n",
    "        # set at end\n",
    "        self.w = None\n",
    "    \n",
    "    '''\n",
    "    #\n",
    "    # forward pass - called to classify - doesn't work\n",
    "    #\n",
    "    def forward(self, xTr, x):\n",
    "        #w = (self.beta.T @ self.xTr).float()\n",
    "        w = (self.beta.T @ xTr)\n",
    "        pred = (w @ x.T)\n",
    "        pred = pred + self.b\n",
    "        pred = torch.sign(pred)\n",
    "        return pred\n",
    "    '''\n",
    "    \n",
    "    # all params to the function have a purpose.....\n",
    "    # [Sigma(beta_i.phi_x_i)].T phi(x) +b\n",
    "    # = Beta.phi(xTr).T.phi(x) + b\n",
    "    # = Beta. phi(xTr)T.phi(x) +b ????\n",
    "    # = Beta K + b??\n",
    "    # K(X, x_i) = phi(X)phi(x_i_)\n",
    "    def forward(self, xTr, x):\n",
    "\n",
    "        K = computeK(self.kernel_type, xTr, x, self.kpar)\n",
    "        \n",
    "        # pred = beta.K + b??\n",
    "        pred = (K.T @ self.beta) + self.b\n",
    "        return torch.sign(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46f24d0e7b48723bbc00465e9cd35151",
     "grade": false,
     "grade_id": "cell-921592ddd34ba0d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    kernelsvm_loss(kernelizedSVM, kernel_mat, yTr, C)\n",
    "    </pre>\n",
    "    It should implement the loss function described above for the equivalent primal formulation of the dual:\n",
    "    $$ min_{\\beta, b} \\frac{1}{2}\\beta^\\top K\\beta + \\frac{C}{2}\\sum_{i=1}^n {[\\max(1-y_i(\\beta^\\top k_i+b),0)]}^2$$\n",
    "  You are given a KernalizedSVM module (<code>kernelizedSVM</code>) which you defined above, the kernel (<code>kernel_mat</code>), the training labels (<code>yTr</code>), and the regularizatin paramater (<code>C</code>). \n",
    " \n",
    "Note that this function <b>requires no loops</b>, and that you may find two functions especially helpful \n",
    "* <code>F.relu(x)</code> computes the <code>max(x,0)</code> in a way that allows for our optimizers to work (F is torch.nn.Functional, a library imported above) \n",
    "* <code>torch.square(x)</code> Returns a new tensor with the square of the elements of input.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21ffe0f7dfbee4627ab16cdc42aba416",
     "grade": false,
     "grade_id": "cell-f41db9a4afd212a2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kernelsvm_loss(kernelizedSVM, kernel_mat, yTr, C):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # yTr = labels for the x's we are training\n",
    "    # kernel_mat = rows from Kernel Matrix for the x's we are testing (i.e. same as computeK(xTr,x) )\n",
    "    pred    = yTr * ((kernel_mat @ kernelizedSVM.beta) + kernelizedSVM.b)\n",
    "\n",
    "    z       = 1 - pred\n",
    "    loss    = torch.square(F.relu(z))\n",
    "    \n",
    "    # strategy 1 (below) only works with mean (makes sense as passing a load of datapoints?)\n",
    "    loss    = C * 0.5 * torch.mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d5936d2d00864188935e053b2ae666b",
     "grade": false,
     "grade_id": "cell-640ffe0d367df416",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<p>\n",
    "    Implement the function \n",
    "    <pre>\n",
    "    dualSVM(xTr, yTr, kernel_type, num_epochs, C, kpar, lr)\n",
    "    </pre>\n",
    "    It should use your functions <code><b>kernelsvm_loss</b></code>, <code><b>computeK</b></code>, and <code><b>KernelizedSVM</b></code> to solve the SVM dual problem of an SVM specified by a training data set (<code><b>xTr,yTr</b></code>), a regularization parameter (<code>C</code>), a kernel type (<code>ktype</code>) and kernel parameter (<code>lmbda</code>), to be used as kpar in Kernel construction. This will once again be a training loop similar to the primalSVM above. You should return a lambda function <code>svmclassify</code> that produces a forward pass of your trained model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35c54bf201bc07c5b33a8eb6d7b02676",
     "grade": false,
     "grade_id": "cell-5da5e62eefa74972",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dualSVM(xTr, yTr, kernel_type, num_epochs=100, C=1, lmbda=0, lr=1e-3):\n",
    "    # YOUR CODE HERE\n",
    "    print_freq=100\n",
    "    dim = yTr.shape[0]\n",
    "    \n",
    "    # setup\n",
    "    svmclassify = KernelizedSVM(dim, kernel_type, lmbda)\n",
    "    optimizer   = optim.SGD(svmclassify.parameters(), lr=lr)\n",
    "    \n",
    "    # compute K - only need to do once\n",
    "    K = computeK(kernel_type, xTr, xTr, lmbda)\n",
    "\n",
    "    # Strategy - TODO\n",
    "    # 1. Just pass all items in one go and beta without yTr > 27/1 this seems to work but only if pass back mean loss\n",
    "    # 2. As 1 but randomising on each epoch - (tried but randomising beta was not obvious in PyTorch, try again if 1.fails)\n",
    "    # 3. Iterate through each x and calc individual hinge loss for SGD - also tried but need to revisit (worked for Primal)\n",
    "    for epoch in range(num_epochs):\n",
    "        # calculate loss for current item\n",
    "        h_loss = kernelsvm_loss(svmclassify, K, yTr, C)\n",
    "        \n",
    "        # get reguariser\n",
    "        K_beta = K @ svmclassify.beta\n",
    "        regulariser = 0.5 * (svmclassify.beta @ K_beta )\n",
    "        \n",
    "        # compute overall loss\n",
    "        loss = regulariser + h_loss \n",
    "            \n",
    "        # backpropagate, optimise and zero the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        if (epoch + 1) % print_freq == 0:\n",
    "            print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "        \n",
    "    svm_lmbda = lambda x: svmclassify(xTr, x)\n",
    "    return svm_lmbda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1695b107fe556c97f22f6285113c5867",
     "grade": false,
     "grade_id": "cell-4613065c19e1631f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>\n",
    "<p>Now we try the SVM with RBF kernel on the spiral data. If you implemented it correctly, train and test error should be close to zero.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xTe,yTe=spiraldata()\n",
    "\n",
    "# poly kernel parameters that don't blow up vvv\n",
    "# ktype=\"poly\"\n",
    "# svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=10, C=0.1, lmbda=0.05, lr=1e-4)\n",
    "# visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# linear kernel parameters that also don't blow up vvv\n",
    "# ktype=\"linear\"\n",
    "# svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=10, C=0.1, lmbda=0.05, lr=1e-4)\n",
    "# visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# rbf kernel with parameters that achieve perfect accuracy vvv \n",
    "ktype=\"rbf\"\n",
    "svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=1000, C=100, lmbda=100, lr=1e-3)\n",
    "#svmclassify=dualSVM(xTr, yTr, kernel_type=ktype, num_epochs=1000, C=100, lmbda=100, lr=1e-3)\n",
    "\n",
    "# I have commented this put in autograder\n",
    "visclassifier.visclassifier(svmclassify,xTr,yTr)\n",
    "\n",
    "# compute training and testing error\n",
    "predsTr=svmclassify(xTr)\n",
    "\n",
    "trainingerr=torch.mean((torch.sign(predsTr.flatten())!=yTr).float())\n",
    "print(\"Training error: %2.4f\" % trainingerr)\n",
    "\n",
    "predsTe=svmclassify(xTe)\n",
    "testingerr=torch.mean((torch.sign(predsTe.flatten())!=yTe).float())\n",
    "print(\"Testing error: %2.4f\" % testingerr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c0aa2df4d10f8db8e37cedd2cba48bd4",
     "grade": false,
     "grade_id": "cell-ca3b1b5cd617938c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing Hint</h3> Create a dataset where you know what some of the optimal values of $\\alpha$ will be, and test to make sure that the solution gets those values of $\\alpha$ correct (recall from the lecture that the $\\alpha$ values associated with certain data points are guaranteed to have a specific optimal value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "63dde3631e6115c1d1f8686a56e28978",
     "grade": true,
     "grade_id": "cell-bd5d833fe168a7a5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 6: testCase_dualSVM_easy_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ba6613d659856839ad586e256ae539ec",
     "grade": true,
     "grade_id": "cell-567ead8163bdad82",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 7: testCase_dualSVM_hard_dataset\n",
    "# --------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies a hard example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ab95808c1c72a42df602edc932ab9ad",
     "grade": true,
     "grade_id": "cell-d6289896907469bc",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 8: testCase_dualSVM_hard_dataset2\n",
    "# ---------------------------------------------\n",
    "# This tests whether the function from dualSVM correctly classifies an even harder example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eec0f599a485e74385a96c48bfe6b594",
     "grade": false,
     "grade_id": "cell-d482c8b79d383b69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "SVMs are pretty sensitive to hyper-parameters. We ask you to implement a cross-validation function. <code>cross_validation</code> which takes training data <code>xTr</code>, training labels <code>yTr</code>, validation data <code>xValid</code>, validation labels <code>yValid</code>, kernel type <code>ktype</code>, list of possible C values <code>CList</code>, list of lambda values for kernel generation <code>lmbdaList</code>, and list of learning rates <code>lr_list</code>.\n",
    "\n",
    "Note that we don't have <code>epochs</code> as a hyper-parameter to tune even, though the number of epochs we train on can vastly change the performance of our model. Generally we train with gradient descent <b>until convergence</b> (when train/validation loss stop decreasing); therefore, something you can do to get a good idea of what amount of epochs you need is plot [epoch number x (training,validation)] loss! <b>This convergence part will not be tested</b>, but is something that might help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b93bf73719fa0a5fc81d8e9493b7cf61",
     "grade": false,
     "grade_id": "cell-523f3c6481d46ac1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList,lr_List):\n",
    "    \"\"\"\n",
    "    function bestC,bestLmbda,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList);\n",
    "    Use the parameter search to find the optimal parameter,\n",
    "    Individual models are trained on (xTr,yTr) while validated on (xValid,yValid)\n",
    "    \n",
    "    Input:\n",
    "        xTr      | training data (nxd)\n",
    "        yTr      | training labels (nx1)\n",
    "        xValid   | training data (mxd)\n",
    "        yValid   | training labels (mx1)\n",
    "        ktype    | the type of kernelization: 'rbf','polynomial','linear'\n",
    "        CList    | The list of values to try for the SVM regularization parameter C (ax1)\n",
    "        lmbdaList| The list of values to try for the kernel parameter lmbda- degree for poly, inverse width for rbf (bx1)\n",
    "        lr_list  | The list of values to try for the learning rate of our optimizer\n",
    "    \n",
    "    Output:\n",
    "        bestC      | the best C parameter\n",
    "        bestLmbda  | the best Lmbda parameter\n",
    "        bestLr     | the best Lr parameter\n",
    "        ErrorMatrix| the test error rate for each given (C, Lmbda Lr) tuple when trained on (xTr,yTr) and tested on (xValid,yValid)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # setup params\n",
    "    ErrorMatrix = torch.zeros((len(CList), len(lmbdaList), len(lr_List)))\n",
    "    bestC, bestLmbda, bestLr = None, None, None\n",
    "    bestTestingErr = np.inf\n",
    "    \n",
    "    NUM_EPOCHS = 100\n",
    "    dbg = False\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, C in enumerate(CList):\n",
    "        for j , lmbda in enumerate(lmbdaList):\n",
    "            for k, lr in enumerate(lr_List):\n",
    "                #print('Executing', i,j,k)\n",
    "                svmclassify=dualSVM(xTr, yTr, ktype, NUM_EPOCHS, C, lmbda, lr)\n",
    "                \n",
    "                predsTe = svmclassify(xValid)\n",
    "                testingerr = torch.mean((torch.sign(predsTe.flatten())!=yValid).float())\n",
    "                ErrorMatrix[i,j,k] = testingerr\n",
    "                \n",
    "                if testingerr < bestTestingErr:\n",
    "                    bestC = C\n",
    "                    bestLmbda = lmbda\n",
    "                    bestLr = lr\n",
    "                    bestTestingErr = testingerr\n",
    "                if(dbg): print(i,j,k, C, lmbda, lr, testingerr, ErrorMatrix[i,j,k], bestTestingErr)\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "\n",
    "    print('Best C, Lambda, LR:', bestC, bestLmbda, bestLr, 'Error:', bestTestingErr, 'Epochs:', NUM_EPOCHS, 'Time (s):', total_time)\n",
    "    \n",
    "    return bestC,bestLmbda,bestLr,ErrorMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "29067c8be390014b6702de8684179a40",
     "grade": false,
     "grade_id": "cell-48b1ca185eae90d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Testing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTr,yTr,xValid,yValid=spiraldata(100)\n",
    "CList=(2.0**np.linspace(-1,5,7))\n",
    "lmbdaList=(np.linspace(0.1,0.5,5))\n",
    "lrList=(np.linspace(0.001,0.005,5))\n",
    "\n",
    "bestC,bestLmbda,bestLr,ErrorMatrix = cross_validation(xTr,yTr,xValid,yValid,'rbf',CList,lmbdaList,lrList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bd2f48dc96555450ceedd74beb8c0cbf",
     "grade": true,
     "grade_id": "cell-80f8696818cafe2c",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 9: testCase_cv\n",
    "# ---------------------------\n",
    "# This tests whether the best hyperparameters found by cross validation are correct for an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5971bcf5be5bd92cc5e24e13027b1b6c",
     "grade": false,
     "grade_id": "cell-8d9a00ede954a98e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h3>Competition</h3>\n",
    "\n",
    "\n",
    "We ask you to implement function autosvm, which given xTr and yTr, splits them into training data and validation data, and then uses a hyperparameter search to find the optimal hyper parameters. \n",
    "\n",
    "Function autosvm should return a function which will act as a classifier on xTe.\n",
    "\n",
    "You have a 5 minute time limit on multiple datasets, each dataset having different optimal hyperparameters, so you should strive for a good method of finding hyperparameters (within the time limit) instead of just trying to find a static set of good hyperparameters. \n",
    "\n",
    "You will get full credit for the competition if you can beat the base benchmark of <b>46% error</b> (you will get partial credit if you beat <b>50% error</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cbcd2378e6c43c6808d605b350e74351",
     "grade": false,
     "grade_id": "cell-2fbc5dbfb2b45cd3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# competition Dual SVM - May use different optimiser\n",
    "# 1. Use a diff optimiser\n",
    "# 2. Break out when loss stabilises?\n",
    "\n",
    "def dualSVM_comp(xTr, yTr, kernel_type, num_epochs=100, C=1, lmbda=0, lr=1e-3):\n",
    "    # YOUR CODE HERE\n",
    "    print_freq=100\n",
    "    dim = yTr.shape[0]\n",
    "    \n",
    "    # setup\n",
    "    svmclassify = KernelizedSVM(dim, kernel_type, lmbda)\n",
    "    #print('using SGD optimizer')\n",
    "    optimizer   = optim.Adam(svmclassify.parameters(), lr=lr, amsgrad=False)\n",
    "    \n",
    "    # compute K - only need to do once\n",
    "    K = computeK(kernel_type, xTr, xTr, lmbda)\n",
    "\n",
    "    # epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # calculate loss for current item\n",
    "        h_loss = kernelsvm_loss(svmclassify, K, yTr, C)\n",
    "        \n",
    "        # get reguariser\n",
    "        K_beta = K @ svmclassify.beta\n",
    "        regulariser = 0.5 * (svmclassify.beta @ K_beta )\n",
    "        \n",
    "        # compute overall loss\n",
    "        loss = regulariser + h_loss \n",
    "            \n",
    "        # backpropagate, optimise and zero the gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        #if (epoch + 1) % print_freq == 0:\n",
    "        #    print('epoch {} loss {}'.format(epoch+1, loss.item()))\n",
    "        \n",
    "    svm_lmbda = lambda x: svmclassify(xTr, x)\n",
    "    return svm_lmbda\n",
    "\n",
    "\n",
    "#\n",
    "# cross validation\n",
    "#\n",
    "def cross_validation_comp(xTr,yTr,xValid,yValid,ktype,CList,lmbdaList,lr_List, epochs):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    i,j,k       = 0,0,0\n",
    "    numCs       = len(CList)\n",
    "    numLmbda    = len(lmbdaList)\n",
    "    numLr       = len(lr_List)\n",
    "    ErrorMatrix = torch.zeros((numCs,numLmbda,numLr))\n",
    "    bestC, bestLmbda, bestLr = None, None, None\n",
    "    bestTestingErr = np.inf\n",
    "    dbg = False\n",
    "    \n",
    "    for C in CList:\n",
    "        j=0\n",
    "        for lmbda in lmbdaList:\n",
    "            k=0\n",
    "            for lr in lr_List:\n",
    "\n",
    "                # create classifier and update best values\n",
    "                svmclassify = dualSVM_comp(xTr, yTr, ktype, epochs, C, lmbda, lr)\n",
    "                predsTe = svmclassify(xValid)\n",
    "                testingerr = torch.mean((torch.sign(predsTe.flatten())!=yValid).float())\n",
    "                ErrorMatrix[i,j,k] = testingerr\n",
    "                \n",
    "                if testingerr < bestTestingErr:\n",
    "                    bestC = C\n",
    "                    bestLmbda = lmbda\n",
    "                    bestLr = lr\n",
    "                    bestTestingErr = testingerr\n",
    "                k+=1\n",
    "            j+=1\n",
    "        i+=1   \n",
    "    #raise NotImplementedError()\n",
    "    return bestC,bestLmbda,bestLr,ErrorMatrix\n",
    "\n",
    "\n",
    "#\n",
    "# Auto SVM\n",
    "#\n",
    "def autosvm(xTr,yTr):\n",
    "    \"\"\"\n",
    "    svmclassify = autosvm(xTr,yTr), where yTe = svmclassify(xTe)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    start = time.time()\n",
    "    \n",
    "    NUM_EPOCHS = 200\n",
    "    \n",
    "    N = yTr.shape[0]\n",
    "    train_percent = 0.8\n",
    "    train_amount = int(N * train_percent)\n",
    "\n",
    "    #split data set\n",
    "    xTrain, yTrain = xTr[:train_amount], yTr[:train_amount]\n",
    "    xValid, yValid = xTr[train_amount:], yTr[train_amount:]\n",
    "    \n",
    "    \n",
    "    CList=(2.0**np.linspace(-1,5,7))\n",
    "    lmbdaList=(np.linspace(0.1,0.5,5))\n",
    "    lrList=(np.linspace(0.001,0.005,5))\n",
    "\n",
    "    # get params from cross validation\n",
    "    bestC,bestLmbda,bestLr,ErrorMatrix = cross_validation_comp(xTrain,yTrain,xValid,yValid,'rbf',CList,lmbdaList,lrList, NUM_EPOCHS)\n",
    "\n",
    "    # create classifier\n",
    "    svmclassify=dualSVM_comp(xTr, yTr, 'rbf', NUM_EPOCHS, bestC, bestLmbda, bestLr)\n",
    "    \n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    #print('Best C, Lambda, LR:', bestC, bestLmbda, bestLr, 'Epochs:', NUM_EPOCHS, 'Time:', total_time, 's')\n",
    "    \n",
    "    return svmclassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e00819254810d5fe2b00f4f7ed31f528",
     "grade": true,
     "grade_id": "cell-9db3d9cbb0582401",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden Test 11: competition\n",
    "# ---------------------------\n",
    "# This tests the error rate of your classifier on the competition datasets \n",
    "# (remember each cell in this notebook should run in < 5 minutes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c8561cb24065c7c8ef55483b5a06381c",
     "grade": true,
     "grade_id": "cell-4626183656c14d2e",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Prints out the summary of tests passed/failed."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
